{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BAN200 Week 01 Homework"
      ],
      "metadata": {
        "id": "tDdPniKEu7ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To complete the homework you will need to modify this template by adding Python code and/or text.\n",
        "\n",
        "Before starting the homework, make sure to save a copy of this template to your personal Google Drive. If you haven't saved your own copy, any changes you make will be lost when you close your browser window.\n",
        "\n",
        "To submit your homework: go to \"File\" in the Colab menu bar > select \"Download\" > select \"Download .ipynb\". This will download a \".ipynb\" file to your computer. You must submit this file.\n",
        "\n",
        "The homework is to be completed in groups. It is due at the start of next class.\n",
        "\n",
        "Homework is graded on the following scale:\n",
        "\n",
        "* *100%* -- The assignment was submitted on time, any code runs without errors, and every question is answered correctly.\n",
        "\n",
        "* *80%* -- The assignment was submitted on time, any code runs without errors, and every question is answered. Some questions may be incorrect, but the submission demonstrates an average level of effort and average level of understanding of the material.\n",
        "\n",
        "* *60%* -- The submission demonstrates a below-average level of effort and below-average level of understanding of the material. This is the highest grade that should be given to submissions that are submitted late, have code that throws uncaught errors, or leave some questions unanswered.\n",
        "\n",
        "* *0%* -- No assignment was submitted, or the submission demonstrates little-to-no effort and little-to-no understanding of the material."
      ],
      "metadata": {
        "id": "Ebn9PTsc6C3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1"
      ],
      "metadata": {
        "id": "tYcvY8gEwszK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Represent this tweet as a Python list of word tokens, ignoring any non-alphanumeric characters:\n",
        "```\n",
        "I love McDonalds!\n",
        "```\n",
        "Your list should be stored in a variable `tweet_0`."
      ],
      "metadata": {
        "id": "XXLVXlwxvQAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MhuYLJ9ru3UY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e299aa-3eb3-4ddb-c89a-03c51d44e6da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'love', 'McDonalds!']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "tweet0 = \"I love McDonalds!\"\n",
        "tweet_0 = tweet0.split()\n",
        "tweet_0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2"
      ],
      "metadata": {
        "id": "Gs8s0KVfwvPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Represent this tweet as a Python list of word tokens, ignoring any non-alphanumeric characters:\n",
        "```\n",
        "McDonalds: you are so good ...\n",
        "```\n",
        "Your list should be stored in a variable `tweet_1`."
      ],
      "metadata": {
        "id": "sibMSRSkwvZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Representing the new tweet as a list of word tokens, ignoring non-alphanumeric characters\n",
        "\n",
        "tweet_new = \"McDonalds: you are so good ...\"\n",
        "\n",
        "# Splitting the tweet into words and removing non-alphanumeric characters\n",
        "tweet_1 = [word.strip(\":.,!\") for word in tweet_new.split() if word.strip(\":.,!\").isalnum()]\n",
        "\n",
        "tweet_1\n"
      ],
      "metadata": {
        "id": "DPR15yQRxNm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2897ba1-8a68-4cde-a2fc-99d2d17a3a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['McDonalds', 'you', 'are', 'so', 'good']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3"
      ],
      "metadata": {
        "id": "vuhYPiUjyZqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Represent this tweet as a Python list of word tokens, ignoring any non-alphanumeric characters:\n",
        "```\n",
        "This McDonalds hamburger, it is gross\n",
        "```\n",
        "Your list should be stored in a variable `tweet_2`."
      ],
      "metadata": {
        "id": "Nh2axebEyZqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Representing the new tweet as a list of word tokens, ignoring non-alphanumeric characters\n",
        "\n",
        "tweet_new_2 = \"This McDonalds hamburger, it is gross\"\n",
        "\n",
        "# Splitting the tweet into words and removing non-alphanumeric characters\n",
        "tweet_2 = [word.strip(\":.,!\") for word in tweet_new_2.split() if word.strip(\":.,!\").isalnum()]\n",
        "\n",
        "tweet_2\n"
      ],
      "metadata": {
        "id": "RAGTbXTPyZqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa37c25-5266-44b2-da64-5677a07f89b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'McDonalds', 'hamburger', 'it', 'is', 'gross']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4"
      ],
      "metadata": {
        "id": "rJXKnNWG_Zdr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Represent this lexicon as a Python dictionary called `lexicon`:\n",
        "\n",
        "```\n",
        "+2.1  love\n",
        "+1.8  good\n",
        "-1.5  gross\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "nR2dofHV_dge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Representing the new lexicon as a Python dictionary\n",
        "\n",
        "lexicon = {\n",
        "    \"love\": 2.1,\n",
        "    \"good\": 1.8,\n",
        "    \"gross\": -1.5\n",
        "}\n",
        "\n",
        "lexicon"
      ],
      "metadata": {
        "id": "DUFEM1dpA6VQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44dcb737-c75c-4f96-b1de-e1c3a3c4a310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'love': 2.1, 'good': 1.8, 'gross': -1.5}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5"
      ],
      "metadata": {
        "id": "ozA3GUqDzXpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a for loop to iterate over all of the tokens in `tweet_2` and make them lower case."
      ],
      "metadata": {
        "id": "CzJ1Ht1-zZw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_2 = \"This McDonalds hamburger, it is gross\"\n",
        "\n",
        "lower_case_tokens = []\n",
        "for token in tweet_2.split():\n",
        "    lower_case_tokens.append(token.lower())\n",
        "\n",
        "tweet_2_lower = ' '.join(lower_case_tokens)\n",
        "print(tweet_2_lower)"
      ],
      "metadata": {
        "id": "4prOUtnDzaOE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a37401b-d20d-4d8b-d6d9-8d6ecbec3ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this mcdonalds hamburger, it is gross\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6"
      ],
      "metadata": {
        "id": "JwM5Sph5BbYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a for loop to calculate a sentiment score for `tweet_2` using `lexicon`."
      ],
      "metadata": {
        "id": "BtSJ-32yBbYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_score_tweet_2 = 0\n",
        "\n",
        "for token in tweet_2_lower:\n",
        "    if token in lexicon:\n",
        "        sentiment_score_tweet_2 += lexicon[token]\n",
        "\n",
        "sentiment_score_tweet_2"
      ],
      "metadata": {
        "id": "ykReYpiwBbYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "498026aa-121e-4d7d-ac86-0e4d833cde62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1.5"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7"
      ],
      "metadata": {
        "id": "DBZ1DuR0B0LN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function that takes two parameters -- a list of tokens and a lexicon -- and returns a sentiment score. Test-out your function with `tweet_2`."
      ],
      "metadata": {
        "id": "Eak51Y5gB0LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentiment(tokens, lexicon):\n",
        "\n",
        "    sentiment_score = 0\n",
        "    for token in tokens:\n",
        "        if token in lexicon:\n",
        "            sentiment_score += lexicon[token]\n",
        "    return sentiment_score\n",
        "\n",
        "# Testing the function with tweet_2\n",
        "sentiment_score_tweet_2_test = calculate_sentiment(tweet_2_lower, lexicon)\n",
        "sentiment_score_tweet_2_test\n"
      ],
      "metadata": {
        "id": "CD_9162rB0LP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d85e978-1f1c-44d1-efd5-b3a1372b8f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1.5"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8"
      ],
      "metadata": {
        "id": "tzp8AShL02T7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function that takes a list of tokens as a paramater and makes them lower case. You can repurpose your code from Question 5. Test-out your function with `tweet_1`."
      ],
      "metadata": {
        "id": "Eu1GJe7Z04RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_tokens(tokens):\n",
        "\n",
        "    return [token.lower() for token in tokens]\n",
        "\n",
        "# Testing the function with tweet_1\n",
        "tweet_1_lower = lowercase_tokens(tweet_1)\n",
        "tweet_1_lower\n"
      ],
      "metadata": {
        "id": "I2j6XV3003eK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c82a7e-ede6-44ec-e835-3a04b8934b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mcdonalds', 'you', 'are', 'so', 'good']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9"
      ],
      "metadata": {
        "id": "XbyMF6StxQm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new variable `corpus` that stores the three earlier tweets in a \"list of lists\"."
      ],
      "metadata": {
        "id": "brjrHomcxR3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [tweet_0, tweet_1, tweet_2]\n",
        "\n",
        "corpus"
      ],
      "metadata": {
        "id": "p0D9k0XdzRUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e41323bb-2af3-4a19-aef0-e7425f1fd70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['I', 'love'],\n",
              " ['McDonalds', 'you', 'are', 'so', 'good'],\n",
              " ['This', 'McDonalds', 'hamburger', 'it', 'is', 'gross']]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10"
      ],
      "metadata": {
        "id": "8-6RVuSm-knj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a for loop to iterate over all of the tweets in `corpus` and use your function from Question 8 to make all of the tokens lowercase."
      ],
      "metadata": {
        "id": "obzpKZnn-lv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_lower = []\n",
        "\n",
        "for tweet in corpus:\n",
        "    corpus_lower.append(lowercase_tokens(tweet))\n",
        "\n",
        "corpus_lower"
      ],
      "metadata": {
        "id": "nVPLGpPo-00p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b24861-1c73-4ce5-f3da-2e11550cc0d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['i', 'love'],\n",
              " ['mcdonalds', 'you', 'are', 'so', 'good'],\n",
              " ['this', 'mcdonalds', 'hamburger', 'it', 'is', 'gross']]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 11"
      ],
      "metadata": {
        "id": "0zZUcayuCh3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a for loop and your function from Question 7 to print-out scores for each tweet."
      ],
      "metadata": {
        "id": "S_1GxD65CjRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in corpus_lower:\n",
        "    score = calculate_sentiment(tweet, lexicon)\n",
        "    print(score)\n"
      ],
      "metadata": {
        "id": "5O9AvoNSCtdF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33fb28d6-4b50-40bb-d8ae-4800c4654846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1\n",
            "1.8\n",
            "-1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 12"
      ],
      "metadata": {
        "id": "glJVQiVyDjbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use your function from Question 7 to score the tweet `McDonalds is not great`."
      ],
      "metadata": {
        "id": "H-5hmNnhDlez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the new tweet \"McDonalds is not great\" for sentiment analysis\n",
        "\n",
        "new_tweet = \"McDonalds is not great\"\n",
        "new_tweet_tokens = lowercase_tokens(new_tweet.split())\n",
        "\n",
        "# Calculating the sentiment score for the new tweet\n",
        "sentiment_score_new_tweet = calculate_sentiment(new_tweet_tokens, lexicon)\n",
        "sentiment_score_new_tweet\n"
      ],
      "metadata": {
        "id": "ekJKKy3nDkrD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2540704c-84b3-4d4e-c4ec-bef8ded9ba32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 13"
      ],
      "metadata": {
        "id": "4RYUnxt1DuBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In one or two sentences, explain the pros and cons of our sentiment model."
      ],
      "metadata": {
        "id": "wg8XyVSLEA28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sentiment model is straightforward and efficient, quickly assigning scores to words based on a specified vocabulary. However, it lacks the ability to interpret complexities like negations or sarcasm and relies too heavily on the correctness and completeness of the lexicon, potentially overlooking the sentiment of words not included."
      ],
      "metadata": {
        "id": "_bu-g5h7EO8I"
      }
    }
  ]
}